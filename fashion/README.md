# Fashion MNIST example

This example demonstates a single model with three operations:
*prepare-data*, *train*, and *predict*.

Refer to [`guild.yml`](guild.yml) for details.

## Prepare data

The `prepare-data` operation uses [`prepare_data.py`](prepare_data.py)
to downloads raw Fashion MNIST files and prepares a dataset for
training and validation. It also generates a number of sample images.

In [`guild.yml`](guild.yml), `prepare-data` is defined as:

``` yaml
    prepare-data:
      description: Prepare images for training.
      main: prepare_data --output-dir .
      flags: {}
```

The `main` specification indicates that the `prepare_data` main module
is used to implement the operation. The option `--output-dir` is a
command line argument to the module that indicates that generated
files should be written to the current directory (`.`) - i.e. the
current run directory.

The value for `flags` tells Guild that the operation does not have any
flags and that Guild should not attempt to import flag definitions
from the `prepare_data` module. If this specification is omitted,
Guild imports all of the flags defined in `prepare_data`, which
include flags that the user should not be able to change
(e.g. `--output-dir`).

To run the operation, change to this directory and run:

    $ guild run prepare-data

When the operation completes, you can view the list of generated files
by running:

    $ guild ls

You can alternatively use your file explorer to view generated images:

    $ guild open

Note these last two Guild commands operate on the latest run by
default - in this case, the `prepare-data` run.

## Train the model

The `train` operation uses [`train.py`](train.py) to train the
model.

In [`guild.yml`](guild.yml), `train` is defined as:

``` yaml
    train:
      description: Train classifier from scratch.
      main: train --data-dir data --log-dir . --checkpoint-dir .
      flags: [epochs, lr]
      requires: prepared-data
      label: epochs=${epochs} lr=${lr}
      compare:
        - =lr
        - epoch_loss step as step
        - epoch_loss as loss
        - epoch_acc as acc
```

The `main` specification indicates that the `train` main module
implements the operation. The `--data-dir`, `--log-dir`, and
`--checkpoint-dir` options specify various paths for the operation. As
with `prepare`, the current directory (`.`) is used to ensure that
files associated with the run are located in the run directory.

In this case, `flags` is a list of two flags defined in the `train`
module: *epochs* and *lr* (short for *learning rate*). Other flags
defined in `train` are not available to the user through the
operation.

The `requires` attribute tells Guild that `train` requires files
generated by the resource named `prepared-data`. This resource is
defined later in the Guild file as:

``` yaml
    prepared-data:
      path: data
      sources:
        - operation: prepare-data
          select: .+\.npy
```

When Guild runs `train`, it first resolves the required resource
`prepared-data` and looks for a run associated with the `prepare-data`
operation. Guild creates a sub-directory named `data` in the `train`
run that contains symbolic links to the `npy` files from the
`prepare-data` run. In this way, `train` has access to the Fashion
MNIST dataset.

The `label` attribute tells Guild to label the `train` so that it
includes values for epochs and learning rate. This is handy for
quickly distinguishing training runs.

`compare` tells Guild how to compare this operation with others when
running the `compare` command. The value is a list of *column
specification* that include flag or scalar names.

To run the operation with the default flag values, run:

    $ guild run train

You can specify different values for `lr` and `epochs`. For
example:

    $ guild run train lr=1e-2 epochs=5

You can generate multiple runs by specifying flag value lists. For
example, running with two learning rates:

    $ guild run train lr=[1e-4,1e-3] epochs=5

You can compare the runs using Guild compare:

    $ guild compare

To exit compare, press `q`.

You can ask Guild to optimize hyperparameters using the `--optimizer`
option along with a search space for flags you want to optimize:

    $ guild run train lr=[1e-5:1e-1] epochs=5 \
                --optimizer bayesian --max-trials 5 \
                --background

Guild will generate 5 trials with different learning rates in an
attempt to minimize the training loss.

You can view the runs in Guild compare. Press `r` to refresh the data.
